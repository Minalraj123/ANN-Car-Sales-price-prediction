# -*- coding: utf-8 -*-
"""Car Purchasing Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sK3dwt4W8UzKZ5x1sRnUYa3Vf1XBScT0
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#Reading Dataset
data = pd.read_csv("/content/car_purchasing.csv",encoding='ISO-8859-1') #Reading a File

#returns the first n rows for the object based on position
data.head()

#printing information about the DataFrame
data.info()

data.keys()

# Check if columns exist before dropping
columns_to_drop = ['customer name', 'customer e-mail', 'country']

# Ensure all columns exist in the DataFrame before trying to drop
columns_in_data = data.columns
columns_not_found = [col for col in columns_to_drop if col not in columns_in_data]

if columns_not_found:
    print(f"These columns are not in the data: {', '.join(columns_not_found)}")
else:
    data = data.drop(columns_to_drop, axis=1)
    print(data.head())

data.head()

#information of the data
data.info()

data.describe()

x=data.iloc[:,0:4]
x

y=data.iloc[:,5]
y

from sklearn.preprocessing import StandardScaler

sc=StandardScaler()
x=sc.fit_transform(x)
y=sc.fit_transform(y.values.reshape(-1, 1))

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=52)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

x_train

x_train

x_train.shape

x_test

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import ReLU,LeakyReLU,ELU,PReLU
from tensorflow.keras.layers import Dropout

import tensorflow as tf
yep=tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0.2,verbose=0,baseline=None,restore_best_weights=True,
                                             start_from_epoch=0,patience=2)

model=Sequential()

model.add(Dense(units=10,activation='relu'))

model.add(Dense(units=10,activation='relu'))
#model.add(Dropout(0.2))     #deactive the 2 neuron (to decrese overfitting)

model.add(Dense(1,activation='linear'))

import tensorflow as tf
opt=tf.keras.optimizers.Adam(learning_rate=0.01)

model.compile(optimizer=opt,loss='mean_squared_error',metrics=['mean_absolute_error'])

ANN=model.fit(x_train, y_train,validation_split=0.2,batch_size=50,epochs=70)

model.summary()

ANN.history.keys()

plt.plot(ANN.history['mean_absolute_error'])
plt.plot(ANN.history['val_mean_absolute_error'])
plt.title('model_accuracy')
plt.ylabel('accuarcy')
plt.xlabel('epochs')
plt.legend(['train','test'],loc='upper left')

plt.plot(ANN.history['loss'])
plt.plot(ANN.history['val_loss'])
plt.title('loss')
plt.ylabel('loss')
plt.xlabel('epochs')
plt.legend(['train','test'],loc='upper left')

y_pred=model.predict(x_test)

y_pred

from sklearn.metrics import r2_score
score=r2_score(y_test,y_pred)

score*100